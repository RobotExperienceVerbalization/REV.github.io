<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Combining Episodic Memory and LLMs for the Verbalization of Robot Experiences</title>

    <meta name="author" content="Joana Plewnia">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="data/favicon.ico" type="image/x-icon">
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
<table class="main-table">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <p class="name" style="text-align: center;">
                Combining Episodic Memory and LLMs for the Verbalization of Robot Experiences
            </p>
            <div class="row">
                <div class="col-md-12 text-center">
                    <ul class="list-inline">
                        <br>
                        <li><a href="https://h2t.iar.kit.edu/21_2941.php">Joana Plewnia</a><sup>1</sup></li>
                        <li><a href="https://h2t.iar.kit.edu/21_2372.php">Tamim Asfour</a><sup>1</sup></li>
                        </li>
                    </ul>
                </div>
                <div class="text-center">
                    <a href="//kit.edu">
                        <img src="images/kit_logo.png" class="top-logo"></img>
                        <br/>
                        <sup>1</sup>Karlsruhe Institute of Technology, Germany
                    </a>
                    <br>
                    <br>
                </div>
            </div>
            <br/>
            <br/>
            <div class="row justify-content-md-center">
                <div class="col-md-2 text-center">
                    <a href="https://arxiv.org/abs/2409.17702">
                        <image src="images/paper_small.png" height="60px"></image>
                        <h4><strong>Paper</strong></h4>
                    </a>
                </div>
                <div class="col-md-2 text-center">
                    <a href="https://git.h2t.iar.kit.edu/sw/armarx/skills/verbalization">
                        <image src="images/github.png" height="60px"></image>
                        <h4><strong>Code</strong></h4>
                    </a>
                </div>
            </div>

            <hr/>

            <div class="row">
                <div class="col-md-12 text-center">
                    <h2>Abstract</h2>
                </div>
                <div class="col-md-12">
                    <br>
                    <p>
                        The ability to communicate past experiences is fundamental for intelligent and natural interaction. 
                        Humanoid robots continuously accumulate rich, multi-modal experiential data and must be able to articulate their episodic experiences in natural language to support effective human-robot communication. 
                        Existing approaches either rely on the generalization capabilities of large language models (LLMs), sometimes combined with episodic memory, or the precision of rule-based verbalization systems, 
                        each presenting limitations when used in isolation. In this work, we present a novel hybrid framework that integrates the adaptability of LLMs with the robustness of rule-based methods 
                        and the generalizable structure of memory-based approaches.  
                        Our system implements strategies to retrieve and transform memory representations of past perceptions and actions into natural language responses. 
                        This enables humanoid robots to respond to natural language queries about their experience. Experimental evaluation based on a set of distinct query types demonstrates that our 
                        approach successfully answers 89.4% of episodic memory questions with human-in-the-loop refinement, while reducing token consumption by 97% compared to pure LLM-based methods. 
                        Furthermore, we demonstrate the system's extensibility by leveraging LLMs, such as GPT-4.1, to expand the range of permissible queries through example-based interaction. 
                        The evaluation on our humanoid robot ARMAR-7 performing household tasks validates that our hybrid approach balances response quality with computational efficiency to address 
                        the crucial need for dependable yet flexible verbalization of robot experiences.  
                    </p>
                </div>
            </div>

            <hr/>

            <div class="col-md-12 text-center">
                <h2>Method Overview</h2>
                         <br>
                <div class="row">
                    
                    <div class="col-sm-4">
                        <h4>Verbalization System</h4>
                        <p>
                        The method comprises two main modules: the <strong>Question Answering (QA)</strong> module and the <strong>Question Generation (QG)</strong> module.
                        The QA module utilizes an existing question definition to extract parameters from the input question, leveraging the <em>Question Template</em>, 
                        followed by the sequential execution of the <em>Select</em>, <em>Search</em>, and <em>Evaluate</em> strategies. 
                        The outcome of the evaluation step is used to populate an <em>Answer Template</em>, resulting in a natural language response.
                        The QG module handles previously unseen questions by abstracting them into a <em>Question Template</em> for future matching. 
                        It then defines the corresponding <em>Select</em>, <em>Search</em>, and <em>Evaluate</em>  strategies - 
                        optionally leveraging a large language model (LLM) - and generates a corresponding <em>Answer Template</em>.
                        </p>
                    </div>
                    <div class="col-sm-4 text-center">
                        <br><br><br>
                        <image src="images/MethodOverview.png" height="750px"></image>
                    </div>
                    <div class="col-sm-4">
                        <p>
                        <h4>Question Templates and Match-Checker:</h4>
                        A Question Template is a string pattern with placeholders used to generalize similar questions. 
                        GPT-4o-mini matches input questions to templates at runtime. 
                        If no match is found, the Question Generation module abstracts key elements and creates a new Question Definition, including search and evaluation strategies. 
                        The system provides context (e.g., valid actions, objects) to help identify variable parts. 
                        When a match occurs, GPT-4o-mini extracts parameters to fill the template.
                        The model handles variations like anaphora and tense through context to improve matching and extraction.
                        <br>
                        <br>
                        <h4>Selection Strategy</h4>
                        The Selection strategy retrieves relevant memory entities using a memory ID, which uniquely identifies an entity or memory segment. 
                        For generating new Question Definitions, the model is given brief descriptions of memory segments and uses semantically meaningful entity names to infer relevance. 
                        The robotic framework allows easy access to all snapshots tied to a memory ID. 
                        When a question spans multiple sources, the system selects multiple memory IDs. 
                        For example, answering “Which objects did you see in the dishwasher the last time you opened it?” requires both scene and action history data, retrieved from separate memory servers. 
                        The strategy includes both IDs for coordinated access.
                        <br>
                        <br>
                        <h4>Search Strategy</h4>
                        The Search strategy defines how to traverse memory snapshots selected during Selection, based on the temporal semantics of a question. 
                        For example, phrases like “the last time you...” trigger a reverse chronological search, while “last week” prompts filtering by date. 
                        Supported strategies include reverse or forward search from specific points (LAST, EARLIEST, BEFORE, AFTER), filtering by time range (DURATION), or retrieving the most recent snapshot (CURRENT).
                        <br>
                        <br>
                        <h4>Evaluation Strategy</h4>
                        The Evaluation strategy determines if a memory snapshot can answer a question and extracts the relevant information in human-readable form. 
                        These are implemented as user-defined functions that operate on snapshot content and metadata, enabling domain-specific logic. 
                        When generating new Question Definitions, the language model creates these functions using utility methods and free-form code.

                        The first function checks if a snapshot is relevant; if so, the second extracts the needed data to fill the Answer Template. 
                        Both functions use input parameters, including values from the Question Template. 
                        If multiple entities are involved, the process runs iteratively, passing output from one step into the next.

                        For example, to answer “Which objects did you see in the dishwasher the last time you opened it?”, the strategy first identifies the action of opening the dishwasher, records the time, 
                        and then uses that timestamp to query the scene memory. 
                        If the timing aligns, the system retrieves objects located "in the dishwasher" using semantic matching, then composes the final answer from this data.
                        </p>
                        <br>
                        <br>
                        <h4>Answer Template</h4>
                        The Answer Template is the final step in both question answering and generation. 
                        Similar to a Question Template, it’s a sentence with placeholders for dynamic content—drawn from the input question, the Evaluation output, or both. 
                        The system supports list formatting for natural language flow, using commas and “and” before the final item. 
                        For example, “The last time I [1] I saw [0]” could become “The last time I opened the dishwasher I saw a red mug, a red plate, and a green bowl.” 
                        This templating approach allows the system to generate fluent, context-aware responses based on user queries and episodic memory.
                    </div>
                </div>     
            </div>

            <hr/>

            <div class="col-md-12 text-center">
                <h2>Results Overview</h2>
            </div>
            <div class=""row">
                <div class="col-sm-6 text-center">
                    <br>
                        <image src="images/results_table.png" height="500px"></image>
                </div>
                <div class="col-sm-6">
                    <p>
                            The evaluation revealed that automatically generated question definitions achieved a 53.12% success rate, which improved to 89.41% with human-in-the-loop adjustments. 
                            Simple question types (e.g., L.1, A.5) were accurately handled, while complex ones, especially involving multiple memory servers (C.1–C.3) or temporal reasoning (C.4), posed significant challenges. 
                            Minor manual interventions by expert users, like converting language expressions to internal identifiers, notably enhanced accuracy. 
                            Defining new question types took an average of 37.84 seconds, whereas answering based on pre-defined templates averaged 4.73 seconds and consumed only 326 tokens. 
                            Most processing effort occurred during the offline generation phase.
                            
                            In comparison, a pure LLM-based approach (per Bärmann et al.) achieved 5 fully and 2 partially correct answers out of 12 representative questions, 
                            where our approach generated 4 definitions fully correct on the first try and 5 definitions requiring only minor manual interventions for correct answers. 
                            The LLM-based approach consumed significantly more tokens (48,200/question) and showed higher latency (14.83 s/question). 
                            While the LLM excelled at core action-related queries, it struggled with peripheral or counting-related ones. 
                            Our method outperformed in these areas, avoided partially correct answers, and offered greater consistency and reliability once definitions were established. 
                            However, our system’s main limitation lies in the LLM’s ability to generate complex question definitions and its reliance on a rigid formalism that limits flexibility.
                    </p>
                </div>
            </div>

            <hr/>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                            This website is based on <a href="https://github.com/jonbarron/jonbarron_website">Jon
                            Barron's source code</a>.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>
</html>
